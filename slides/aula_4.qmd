---
title: "Aula 4 - Machine Learning"
subtitle: 'Introdução ao aprendizado de máquina - UEMA 2025'
author: 'Thiago S. F. Silva'
date: 2025-12-15
format: 
    beamer:
        theme: metropolis
        urlcolor: blue
        include-in-header:
          file: custom.tex
---

# Parte I - Introdução

## Revisitando: *machine learning* vs. estatística

- Focado em **predição** 
- Sem valor-p, intervalo de confiança, pressuposições, etc.
- Pouca ou nenhuma **interpretabilidade** do modelo.
- Sem preocupação com o 'mínimo modelo' ou parsimônia. 
- Os melhores algoritmos de ML não requerem linearidade, normalidade, independência, etc.
- Bons métodos estatísticos (ex. modelos lineares) são inferiores para ML.

## Exemplo: Boston Housing Price

\scriptsize

```{r BHPload, echo=FALSE}
library(tidyverse)
library(tidymodels)
library(tidyfit)
data <- MASS::Boston
str(data)
```

## Regressão Linear vs Random Forests

```{r lmvsrf, echo=FALSE}
# For reproducibility
set.seed(128)
ix_tst <- sample(1:nrow(data), round(nrow(data)*0.1))

data_trn <- data[-ix_tst,]
data_tst <- data[ix_tst,]

model_frame <- data_trn %>% 
  regress(medv ~ .,
          OLS = m("lm"),
          RF = m("rf")
          )

model_frame %>% 
  # Generate predictions
  predict(data_tst) %>% 
  # Calculate RMSE
  yardstick::rmse(truth, prediction)
```
## Terminologia

- **Treinamento:** cálculo dos parâmetros do modelo a partir dos dados. 
- **Tuning (otimização):** otimização dos *hyperparâmetros*  do modelo.
- **Validação:** avaliação do modelo durante o *tuning*.
- **Teste:** avaliação final do modelo usando dados de teste.
- **Target (Alvo):** variável dependente / alvo da predição.
- **Feature (feição):** variáveis preditoras/variáveis independentes.

## Terminologia

- **Classificação:** qualquer problema/modelo que produz resultados *categóricos*.
- **Regressão:** qualquer problema/modelo que produz resultados *contínuos*.

No caso específico de visão computacional:

![](../figs/segvsclasvsdecjpg.jpg)

# Parte II - Aplicando Machine Learning

## Machine learning no R

- O pacote [`tidymodels`](https://www.tidymodels.org/)
- O livro ['Tidy Models with R'](https://www.tmwr.org/).
- O livro mais antigo [`Applied Predictive Modeling with R`](https://link.springer.com/book/10.1007/978-1-4614-6849-3) ainda é bom para *entender* ML.
- O website do pacote Python [Scikit-learn](https://scikit-learn.org/stable/) é excelente para explicações sobre algoritmos específicos, independente da linguagem.

## Os passos de uma análise de ML

1. ~~Preparação dos dados~~
2. ~~Análise descritiva~~
3. ~~Visualização de dados~~
4. **Divisão dos dados em *treinamento* e *teste* **
5. **Escolha do algoritmo**
5. **Otimização e validação do modelo** 
6. **Teste do modelo**

## Algoritmos de Machine Learning

Existem literalmente dezenas de algoritmos de machine learning. Vamos focar nos mais comuns:

- Árvores de decisão (*decision trees*)
- *Support Vector Machine* (SVM)
- Métodos de *Ensemble*:
    - Random Forests (*bagging*)
    - LightGBM (*boosting*)

## Árvores de Decisão

\small

Um dos métodos mais 'básicos' de ML. Suas vantagens são:

- Fáceis de compreender
- Robustos com relação aos dados
- Não-Paramétricos
- Funcionam pra Classificação e Regressão.

Desvantagens:

- Árvores de decisão são propensas ao *overfitting*, e requerem métodos de "poda" (*pruning*) para manter a generalidade.
- As predições não são contínuas.
- Sensíveis a dados desbalanceados.

## Overfitting

\small 

Algoritmos de ML podem aprender "bem demais". Isso é chamado de *overfitting* (ajuste excessivo). Normalmente detectado por uma acurácia alta durante a otimização, e baixa durante o teste.

![](../figs/Overfitting.jpg){width=50% fig-align="center"}

## Dados desbalanceados

Quando uma ou mais classes dominam a quantidade de amostras sobre as outras.

```{r imbalanceplot}
set.seed(46)
library(ggplot2)
df <- data.frame(x = runif(50,0,1),y = runif(50,0,1), classe=c(rep('C',89),rep('A',5),rep('B',6)))

ggplot(df,aes(x,y,color=classe)) + geom_point(size=3) + theme_gray(base_size = 25)
```
## Árvores de Decisão

![](../figs/decision-tree.png)

[https://www.youtube.com/watch?v=ZVR2Way4nwQ](https://www.youtube.com/watch?v=ZVR2Way4nwQ)

## Classificando o dataset *iris*

\small

`Petal.Width`,`Petal.Lenght`,`Sepal.Width`,`Sepal.Length` são as nossas *features*. 
```{r headiris}
head(iris)
```

## Classificando o dataset *iris*

Nosso nó inicial contém todas as amostras. Queremos achar um limiar para um dos quatro *features*  que maximiza a separação entre os grupos. 

Como 'medir' essa separação?

**Pureza do Nó (Node Purity):** quão 'misturado' é o dataset após a separação?

## Classificando o dataset *iris*

**Exemplos (C = número de classes):**

\small

:::: {.columns}

::: {.column width="50%"}

**Índice de Pureza de Gini**

$Gini = 1 - \sum_{i=1}^{C}{p_i * (1-p_i)}$

- Máximo de Pureza: 0
- Mínimo: $(C-1)/C$
:::

::: {.column width="50%"}
**Índice de Entropia de Shannon**

$Entropia = - \sum_{i=1}^{C}{p_i * log(p_i)}$

- Mínimo de Entropia: 0
- Máximo: $log(C)$
:::

::::

## Classificando o dataset *iris*

Então podemos testar diferentes limiares de separação para diferentes *features*, e escolher aquele que maximiza a pureza (minimiza o índice Gini).

## Classificando o dataset *iris*

```{r ggpairsiris}
library(GGally)
ggpairs(iris, columns = 1:4, ggplot2::aes(colour = Species), upper = 'blank')
```

## Classificando o dataset *iris*

\small

Para separar a espécie *setosa*, basta um limiar de `Petal.Length > 2.5`.

```{r ggplotiris1}
ggplot(iris,aes(Petal.Length,Petal.Width,colour = Species)) + 
    geom_point() + 
    geom_vline(xintercept = 2.5) +
    theme_gray(base_size = 25)
```
## Classificando o dataset *iris*

Diferenciação entre *versicolor* e *virginica* é mais complexa. Pra isso serve o computador!

```{r rodarpart1, echo=TRUE}
library(rpart)
library(rpart.plot)
model1 <- rpart(Species ~ ., data = iris,
                method = "class")
```
## Classificando o dataset *iris*

\footnotesize

```{r plotrpart1, echo=TRUE}
rpart.plot(model1, box.palette = "RdBu", shadow.col = "gray",
           fallen.leaves = TRUE)
```

## Classificando o dataset *iris*

```{r rpartdecbounds}
ggplot(iris,aes(Petal.Length,Petal.Width,colour = Species)) + 
    geom_point() + 
    geom_vline(xintercept = 2.5) +
    geom_hline(yintercept=1.75) +
    theme_gray(base_size = 25)
```

## Classificando o dataset *iris*

Poderíamos ir além?

```{r rpartoverfit, echo=TRUE, eval=FALSE}
model2 <- rpart(Species ~ ., 
                data = iris,
                method = "class", 
                control = rpart.control(cp = 0,
                                        minsplit = 2))
```

## Classificando o dataset *iris*

```{r rpartoverfitview, echo=FALSE, eval=TRUE}
model2 <- rpart(Species ~ ., data = iris, method = "class", control = rpart.control(cp = 0, minsplit = 2))
rpart.plot(model2, box.palette = "RdBu", shadow.col = "gray",
           fallen.leaves = TRUE)
```

## Classificando o dataset *iris*

Nesse novo modelo, mudamos dois *hiperparâmetros*:

- `cp`: controla o grau de complexidade da árvore
- `minsplit`: numero mínimo de observações após uma separação.

```{r rpartaccuracy}
prediction1 <- predict(model1, type = "class")
prediction2 <- predict(model2, type = "class")

confMat1 <- table(actual = iris$Species, predicted = prediction1)
confMat2 <- table(actual = iris$Species, predicted = prediction2)

accuracy1 <- sum(diag(confMat1)) / sum(confMat1)
accuracy2 <- sum(diag(confMat2)) / sum(confMat2)
print(paste0("Acurácia M1 (cp = 0.01, minsplit = 20): ", accuracy1*100,'%'))
print(paste0("Acurácia M2 (cp = 0, minsplit = 2 ): ", accuracy2*100,'%'))
```

Qual o melhor modelo?

## De árvores para florestas

As árvores de decisào funcionam bem com problemas simples, mas:

- Perdem eficiência em problemas complexos

- Tem uma grande tendência ao overfitting

## Random Forests

**Random Forests (Florestas Aleatórias):** um modo de deixar o uso de árvores de decisão mais robusto e poderoso.

Criado por Leo Breiman em 2001:  [https://doi.org/10.1023/A:1010933404324](https://doi.org/10.1023/A:1010933404324)

![](../figs/RF_citations.jpg)

## Random Forests

A ideia é incrivelmente simples:

- Ao invés de uma árvore gigante, crie diversas pequenas árvores
- Cada árvore prevê corretamente só uma pequena parte dos dados (*weak learner*)
- A classificação/regressão final é o *consenso* entre todas as árvores.
- Em ML, essa técnica é chamada de *bagging* (bootstrap + aggregation).

## Random Forests

Esse 'truque' de usar vários modelos para gerar uma predição final é chamado de *ensemble* (conjunto). 

Ele pode ser também utilizado para combinar resultados de diferentes algoritmos, buscando assim maior robustez.

[https://www.youtube.com/watch?v=v6VJ2RO66Ag&t=61s](https://www.youtube.com/watch?v=v6VJ2RO66Ag&t=61s)

## Random Forests

O algoritmo *Random Forest* é extremamente robusto e poderoso, e fácil de treinar e otimizar. Os principais *hiperparâmetros* a ser otimizados são:

- **`n_estimators`**: quantas árvores? Mais árvores = melhores modelos = maior tempo de computação.
- **`max_features`:** o número máximo de features considerado a cada separação, O padrão é `sqrt(total_features)`. Serve para criar árvores mais diferentes.
- `max_depth`:** profundidade máxima de cada árvore (reduz overfitting)
- `min_samples_split`: número mínimo de amostras que um nó deve ter pra ser considerado pra mais uma separação.
- `min_samples_leaf`: número mínimo de amostras nas folhas (nós terminais). 

## Um exemplo usando o pacote `ranger`

Existem mais de um pacote no `R` que implementam o Random Forest:

- `ranger`
- `randomForest`
- `partykit`
- `aorsf`

Vamos usar o pacote `ranger` para um exemplo.

## Quais dados vamos modelar?

**Breast Cancer Winsconsin:** um dataset com dados reais derivados de imagens de células cancerígenas, utilizados para diagnosticar entre maligno/benigno.

Disponível no R no pacote `mlbench`, um pacote com vários datasets clássicos usados como *benchmark* no mundo do ML.

[Breast Cancer Wisconsin (Diagnostic)](https://doi.org/10.24432/C5DW2B)

## Quais dados vamos modelar?

```{r headbc, echo = TRUE, eval=FALSE}
data("BreastCancer", package = "mlbench")
head(BreastCancer)
str(BreastCancer)
```

## Primeiro Passo - Inspecionando os dados

```{r inspbc, echo = TRUE, eval=FALSE}
library(GGAlly)
summary(BreastCancer)
ggpairs(BreastCancer, columns = c(2:5,11))
ggpairs(BreastCancer, columns = c(6:10,11))

```

## Segundo Passo - Separando os dados em treino e teste

```{r splitbc, echo = TRUE, eval=FALSE}
set.seed(42)
nrow(BreastCancer) * 0.7
row_inds <- c(1:nrow(BreastCancer))
train_inds <- sample(row_inds,490, replace = FALSE)

train_df <- BreastCancer[train_inds,-1]
test_df <- BreastCancer[-train_inds,-1]
```

## Terceiro Passo - Ajuste do modelo
```{r fitbc, echo = TRUE, eval=FALSE}
library(ranger)

m1 <- ranger(Class ~ ., data = train_df)

m1$prediction.error
```

## Quarto passo - otimização do modelo

\small

```{r tunebc, echo = TRUE, eval=FALSE}
tune_df <- data.frame(numtree_vals = c(10, 50,100,500,1000),
                      error = NA)
tune_df

for(n in c(1:nrow(tune_df))){
    temp_mod <- ranger(Class ~ ., data = train_df,
                       num.trees = tune_df$numtree_vals[n])
    tune_df$error[n] <- temp_mod$prediction.error
} 

tune_df
```

## Quarto passo - otimização do modelo

\small

```{r tunebc2, echo = TRUE, eval=FALSE}
tune_df <- data.frame(
    numtree_vals = rep(c(10, 50,100,500,1000),3),
    mtry_vals = rep(c(2,4,6),3, each=5), 
    error = NA
    )
tune_df

for(n in c(1:nrow(tune_df))){
    temp_mod <- ranger(Class ~ ., data = train_df,
                       num.trees = tune_df$numtree_vals[n],
                       mtry = tune_df$mtry_vals[n])
    tune_df$error[n] <- temp_mod$prediction.error
} 

tune_df

which(tune_df$error == min(tune_df$error))

tune_df[5,]
```

## Quinto Passo - Teste do modelo

```{r testbc, echo = TRUE, eval=FALSE}
final_mod <- ranger(Class ~ ., data = train_df,
                       num.trees = 1000,
                       mtry = 2)
pred <- predict(final_mod,test_df)

conf_mat <- table(test_df$Class, pred$predictions)

acuracia <- sum(diag(conf_mat)) / sum(conf_mat)

erro <- 1-acuracia

```
# Parte III - Conceitos Adicionais

## `set.seed()` ?

O comando `set.seed()` fica a 'semente'do gerador de numeros aleatorios.

- É muito dificuil criar um mecanismo computacional para gerar numeros realmente aleatórios
- Computadores usam algortimos que geram números pseudo-aleatórios
- A partir de um valor inicial, aplicam uma série de cálculos instáveis, que resultam em numeros completamente diferentes mesmo se a semente é bem similar.
- Sem uma especificação de semente, o computador usa o horário.

## `set.seed()` ?

```{r setseed, echo=TRUE}
set.seed(42)
runif(5,0,1)

set.seed(43)
runif(5,0,1)
```

## `set.seed()` ?

Especificar o `set.seed()` garante a reprodutibilidade dos resultados.


## Engenharia de feições

\small

Pré-processamento das feições para melhorar a performance do modelo.

- Normalização/Padronização: equaliza a escala entre as feições contínuas. 

- Transformações não-lineares: logaritmos, raízes, potências.

- Combinações de variáveis: multiplicações, razões. Ex. NDVI.

- Desmembramento de variáveis: separa informações complexas, como decompor datas em dia, mês, ano, semestre, trimestre, etc.

- *One-Hot Encoding*: converter variáveis categóricas em variáveis *dummy* numéricas.

- *Label Encoding*: substuir categorias por números inteiros (menos impprtante no R).

- Imputação: preenchimento de valores vazios usando alguma regra.

## Otimizando o modelo - validação cruzada

- Sempre queremos testar o modelo com dados 'novos'.

- A separação entre treino e teste já sacrifica parte dos dados.

- Se precisarmos separar a amostra de treino de novo entre treino e validação, perdemos ainda mais.

## Otimizando o modelo - validação cruzada

*k- (ou v-) fold cross validation*: separação dos dados de treino em 'blocos' (*folds*). 

Ex: 5-fold cross-validation

1. Divida os dados de treino aleatoriamente em 5 blocos iguais.
2. Junte quatro dos blocos em uma tablela e ajuste o modelo.
3. Use o quinto bloco para validar o modelo, calculando uma métrica de erro.
4. Repita mais 4 vezes, cada vez deixando um bloco de fora.
5. Faça a média dos 5 erros calculados.

Vantagens: 

- Todos os dados de treino são usados para treino e para validação.
- A cada combinação de hiperparametros testada, você estima um erro **médio**. 
## Otimizando o modelo - validação cruzada

![https://scikit-learn.org/stable/modules/cross_validation.html](../figs/cross_validation.png){width=80% fig-align='center'}
## Alternativa: *jacknife* ou *LOOCV*

**Leave-One-Out Cross Validation (LOOCV):**também conhecido como *jacknife*. Assumindo que você tem $N% observações, o procedimento é:

1) Retire a observação $n=1$ da tabela.
2) Ajuste o modelo com as observações restantes.
3) Faça a predição da obs que ficou fora e calcule o erro.
4) Repita para $n = {2,...,N}$.
5) Faça a média dos $N$ erros.

Útil quando o dataset é pequeno, mas custoso computacionalmente se o dataset é grande. 

## Métricas de Erro/Acurácia

- Quantificar o erro de predição é um dos aspectos essencias do processo de ML. 

- Como se o modelo fosse uma prova, e a métrica fosse a nota da prova. 

- Quando melhor a nota, maior o aprendizado (em teoria)

## Métricas de Erro/Acurácia

Existem dezenas de métricas de erro, vamos focar apenas em algumas. Também são chamadas de *loss functions* ou *scoring functions*.

- Acurácia Simples
- Índice Kappa
- AUC/ROC
- Precision/Recall/F1 score
- RMSE e MAE

## A matriz de confusão

Todas as métricas de classificação são de alguma maneira relacionadas à matriz de confusão:

![](../figs/conf_mat.png){fig-align="center"}


## A matriz de confusão

Para um problema multi-classe, temos que considerar cada classe:

|              | **Classe A** | Classe B | Classe C | Classe D |
|--------------|----------|--------------|----------|----------|
| **Classe A** | _TP_     | _FN_         | _FN_     | _FN_     |
| Classe B     | TN       | TN           | TN       | TN       |
| Classe C     | TN       | TN           | TN       | TN       |
| Classe D     | TN       | TN           | TN       | TN       |

## A matriz de confusão

Para um problema multi-classe, temos que considerar cada classe:

|              | Classe A | **Classe B** | Classe C | Classe D |
|--------------|----------|--------------|----------|----------|
| Classe A     | TN       | TN           | TN       | TN       |
| **Classe B** | _FN_     | _TP_         | _FN_     | _FN_     |
| Classe C     | TN       | TN           | TN       | TN       |
| Classe D     | TN       | TN           | TN       | TN       |

## A matriz de confusão

Dependendo da métrica, podemos generalizar para:

|              | Classe A | Classe B     | Classe C | Classe D |
|--------------|----------|--------------|----------|----------|
| Classe A     | Correto  | Errado       | Errado   | Errado   |
| Classe B     | Errado   | Correto      | Errado   | Errado   |
| Classe C     | Errado   | Errado       | Correto  | Errado   |
| Classe D     | Errado   | Errado       | Errado   | Correto  |

## Acurácia Simples ou Global

Simplesmente a proporção de acertos (ou erros):


$Acuracia = \frac{TP+TN}{TP+TN+FP+FN}$


## Acurácia Simples ou Global

|              | Classe A | Classe B     |
|--------------|----------|--------------|
| Classe A     | 26       | 3            |
| Classe B     | 6        | 30           |

- Ac. Global = (26 + 30)/(26+3+6+30) = `r round((26 + 30) / (26+3+6+30),3)`
- Ac. Classe A = 26/(26+3) = `r round(26/(26+3),3)`
- Ac. Classe B = 30/(30+6) = `r round(30/(30+6),3)`

## Índice Kappa

Muito usado em sensoriamento remoto, é uma modificação da acurácia que 'desconta' a probabilidade de você acertar por acaso.

$\kappa =\frac {2\times (TP\times TN-FN\times FP)}{(TP+FP)\times (FP+TN)+(TP+FN)\times (FN+TN)}$

## Índice Kappa

|              | Classe A | Classe B     |
|--------------|----------|--------------|
| Classe A     | 26       | 3            |
| Classe B     | 6        | 30           |

```{r}
library(yardstick)
mat <- matrix(nrow = 2, ncol = 2, data = c(26,6,3,30), byrow = T)
k <- round(kap(mat)$.estimate,3)
```
Acurácia Global = `r round((26 + 30) / (26+3+6+30),3)`
Kappa = `r k`.

## ROC/AUC

*ROC* significa *Receiver Operator Curve*: representação visual da performance do modelo. 

*AUC* significa *Area Under the Curve*: área abaixo da curva ROC.

## ROC/AUC

:::: {.columns}

::: {.column width="50%"}

![](../figs/auc_0-5.png)
![](../figs/auc_0-65.png)

:::

::: {.column width="50%"}

![](../figs/auc_0-93.png)
![](../figs/auc_1-0.png)
:::

::::

## Precision, Recall, Sensitivity, Specificty, F1 Score

Métricas mais apropriadas para o *TESTE* do modelo, pois podem ser interpretadas mais sutilmente. Relembrando: 

![](../figs/conf_mat.png){fig-align="center"}

## Precision, Recall, Sensitivity, Specificty, F1 Score

**Precision (TP/TP+FP):** porcentagem de positivos corretos dentre todos os positivos *detectados*. Em SR, a *'acurácia do produtor'*. Maior precisão =  menor chance de falsos positivos.

|              | Classe A | Classe B     |
|--------------|----------|--------------|
| Classe A     | 26       | 3            |
| Classe B     | 6        | 30           |

- Precisão Classe A = 26/(26+6) = `r round(26/(26+3),3)`
- Precisão Classe B = 30/(30+3) = `r round(30/(30+6),3)`

## Precision, Recall, F1 Score

**Recall (TP/TP+FN):** porcentagem de positivos verdadeiros que foram efetivamente identificados. O mesmo que acurácia por classe ou *'acurácia do usuário*. Maior recall = maior chance de detecção de positivos.

|              | Classe A | Classe B     |
|--------------|----------|--------------|
| Classe A     | 26       | 3            |
| Classe B     | 6        | 30           |

- Precisão Classe A = 26/(26+3) = `r round(26/(26+3),3)`
- Precisão Classe B = 30/(30+6) = `r round(30/(30+6),3)`

## Exemplos de aplicação

Caso 1: Deteção de Spam - é melhor deixar um e-mail de Spam cair no seu inbox do que jogar um e-mail real na caixa de Spam.

- A Precisão foca em minimizar Falsos Positivos (FP). Maior precisão significa menos emails de SPAM são marcados como reais, à custa de marcar algus emails reais como spam.


## Exemplos de aplicação

Caso 2: Diagnostico de uma doença - é melhor diagnosticar um paciente sadio como doente do que deixar de diagnosticar um paciente doente.

- O Recall foca em minimizar Falsos Negativos. Maior recall significa que mais pacientes doentes são diagnosticados, à custa de diagnosticar alguns sadios.


## Precision, Recall, F1 Score

Exemplo interativo:

[https://developers.google.com/machine-learning/crash-course/classification/accuracy-precision-recall](https://developers.google.com/machine-learning/crash-course/classification/accuracy-precision-recall)

## F1 Score

F1 Score: uma métrica única que combina Precision e Recall:

$F1=\dfrac{2 \times Precision \times Recall}{Precision + Recall}$

Essa formula prioriza valores balanceados das duas métricas.

## Erros contínuos

Para problemas de *regressão*, os erros são mais simples: quão perto/longe você chegou do valor real?

- Root Mean Squared Error (RMSE): $\sqrt{\dfrac{\sum_{i=1}^{N}(pred_i-obs_i)^2}{N}}$

- Mean Absolute Error (MAE): $\dfrac{\sum_{i=1}^{N}\left|\left(pred_i-obs_i\right)\right |}{N}$